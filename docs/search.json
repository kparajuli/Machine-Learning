[
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this site\n\n1 + 1\n\n[1] 2"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Machine-Learning",
    "section": "",
    "text": "Machine Learning (ML), a revolutionary field within the realm of artificial intelligence, empowers computers to learn from data and make informed decisions without explicit programming. At its core, ML is a paradigm that enables systems to recognize patterns, draw insights, and improve performance over time through experience. Whether it’s predicting stock prices, recognizing speech, or recommending personalized content, machine learning algorithms have permeated various facets of our daily lives. The driving force behind ML’s success lies in its ability to uncover hidden patterns within vast datasets, allowing for the development of models that can generalize and adapt to new information. As we embark on this exploration of machine learning, we delve into the algorithms, methodologies, and applications that are reshaping the landscape of technology and decision-making. Join us on this journey through the intricacies of machine learning, where the convergence of data and computation opens up new frontiers of possibilities.\nThere are Five different aspects of Machine Learning the blog explores:\n\nProbability theory and random variables\nClustering\nLinear and nonlinear regression\nClassification\nAnomaly/Outlier detection\n\nEach of topics are explored more in the tabs above!\nThe source git repository is attached here: Source Github Repository The data sets used are placed in a different repository here: Data Set Repository\nThe following are the list of kaggle projects that served as an inspiration for the explorations.\nReferences\n\nhttps://www.kaggle.com/code/tharakinfinity/k-means-clustering-in-r\nhttps://www.kaggle.com/code/tharakinfinity/hc-clustering\nhttps://www.kaggle.com/code/meepbobeep/intro-to-regression-and-classification-in-r\nhttps://www.kaggle.com/code/pranjalpandey12/simple-to-multiple-and-polynomial-regression-in-r/notebook\nhttps://www.kaggle.com/code/norealityshows/outlier-detection-with-isolation-forest-in-r"
  },
  {
    "objectID": "anomaly.html",
    "href": "anomaly.html",
    "title": "Anomaly/Outlier Detection",
    "section": "",
    "text": "Anomaly detection is a crucial aspect of data analysis that involves identifying patterns in data that do not conform to expected behavior. In this blog post, we’ll explore the fundamentals of anomaly detection in R, covering various techniques, and diving into the unique characteristics of the Isolation Forest algorithm.\n\nAnomaly Detection\nAnomaly detection, also known as outlier detection, is the process of identifying instances in a dataset that deviate significantly from the norm. Anomalies can represent critical information, such as fraudulent transactions, system malfunctions, or defects in manufacturing.\n\n\nTechniques in Anomaly Detection\n\nStatistical Methods\nStatistical methods, such as Z-Score and Modified Z-Score, involve measuring how many standard deviations a data point is from the mean. Points that fall beyond a certain threshold are considered anomalies.\n\n\nDensity-Based Methods\nDensity-based methods, like DBSCAN (Density-Based Spatial Clustering of Applications with Noise), identify regions in the data space with lower density, flagging points in sparse regions as anomalies.\n\n\nApplying Anomaly Detection - Isolation Forest\n\n\nIsolation Forest\nIsolation Forest is an ensemble method based on decision trees, particularly designed for anomaly detection. It isolates anomalies more effectively by exploiting the fact that anomalies are typically isolated instances.\nIsolation Forest has several advantages for anomaly detection:\n\n\nEfficiency: computationally efficient, making it suitable for large data sets\n\nInsensitivity to Multicollinearity: less affected by features that are highly correlated\n\nScalability: performs well in high-dimensional spaces\n\nIsolation Forest works by recursively partitioning the data into subsets until anomalies are isolated. The idea is that anomalies are more likely to require fewer partitions to be separated from the majority of normal instances.\n\n\n\nAttaching package: 'MLmetrics'\n\n\nThe following object is masked from 'package:base':\n\n    Recall\n\n\nFor the purpose of this blog, we will create a sample data instead of using a specific data set.\n\nn &lt;- 1000\nvar1 &lt;- c(rnorm(n, 0, 0.5), rnorm(n*0.1, -2, 1))\nvar2 &lt;- c(rnorm(n, 0, 0.5), rnorm(n*0.1, 2, 1))\n\noutliers &lt;- c(rep(0,n), rep(1, (0.1*n))) + 3\ndata &lt;- data.frame(var1, var2)\nhead(data)\n\n         var1        var2\n1 -0.13751982 -0.43006868\n2  0.38668441  0.11542851\n3 -0.05619019 -0.40391657\n4  0.73102514  0.02602187\n5  0.36584339 -0.44598605\n6 -0.20110447 -0.05275216\n\n\n\nggplot(data, aes(x = var1, y = var2)) + \n  geom_point(shape = 1, alpha = 0.5) +\n  labs(x = \"x\", y = \"y\") +\n  labs(alpha = \"\", colour=\"Legend\")\n\n\n\n\nNext, we create an isolation forest from the solitude package with default parameters\n\ni_forest &lt;- isolation.forest(data, ndim=1, ntrees=10, nthreads=1)\n\n## predict outliers within data set\ndata$prediction &lt;- predict(i_forest, data)\ndata$outlier &lt;- as.factor(ifelse(data$prediction &gt;=0.50, \"outlier\", \"normal\"))\n\n\n#plot data again with outliers identified\nggplot(data, aes(x = var1, y = var2, color = outlier)) + \n  geom_point(shape = 1, alpha = 0.5) +\n  labs(x = \"x\", y = \"y\") +\n  labs(alpha = \"\", colour=\"Legend\")\n\n\n\n\nNext, we adjust the threshold to isolate greater outliers.\n\ndata$outlier &lt;- as.factor(ifelse(data$prediction &gt;=0.55, \"outlier\", \"normal\"))\n\nggplot(data, aes(x = var1, y = var2, color = outlier)) + \n  geom_point(shape = 1, alpha = 0.5) +\n  labs(x = \"x\", y = \"y\") +\n  labs(alpha = \"\", colour=\"Legend\")\n\n\n\n\nWe will also look at different variants of isolation forests.\nDensity Isolation Forest\n\ni_forest2 &lt;- isolation.forest(data, ndim=1, ntrees=10, nthreads=1,\n                              scoring_metric=\"density\")\n\n## predict outliers within data set\ndata$prediction &lt;- predict(i_forest2, data)\ndata$outlier &lt;- as.factor(ifelse(data$prediction &gt;=0.50, \"outlier\", \"normal\"))\n\n\n#plot data again with outliers identified\nggplot(data, aes(x = var1, y = var2, color = outlier)) + \n  geom_point(shape = 1, alpha = 0.5) +\n  labs(x = \"x\", y = \"y\") +\n  labs(alpha = \"\", colour=\"Legend\")\n\n\n\n\nFair-Cut Forest\n\ni_forest3 &lt;- isolation.forest(data, ndim=1, ntrees=10, nthreads=1, \n                              prob_pick_pooled_gain=1)\n\nWarning in isolation.forest(data, ndim = 1, ntrees = 10, nthreads = 1,\nprob_pick_pooled_gain = 1): Passed parameters for deterministic single-variable\nsplits with no sub-sampling. Every tree fitted will end up doing exactly the\nsame splits. It's recommended to set non-random split probabilities to less\nthan 1, or to use the extended model (ndim &gt; 1).\n\n## predict outliers within data set\ndata$prediction &lt;- predict(i_forest3, data)\ndata$outlier &lt;- as.factor(ifelse(data$prediction &gt;=0.50, \"outlier\", \"normal\"))\n\n\n#plot data again with outliers identified\nggplot(data, aes(x = var1, y = var2, color = outlier)) + \n  geom_point(shape = 1, alpha = 0.5) +\n  labs(x = \"x\", y = \"y\") +\n  labs(alpha = \"\", colour=\"Legend\")\n\n\n\n\nWe can see how different methods detect anomalies differently. Based on the context, and research question the choice of anomaly detection methods are going to vary. Since outliers are subjective, assessing the models performance or accuracy is challenging. In conclusion, anomaly detection is a vital component f data analysis, providing insights into unusual patterns and potential issues within data sets. In R, a variety of techniques, including statistical methods, density-based methods and a powerful Isolation Forest algorithm, can be applied for effective anomaly detection. The spotlight on Isolation Forest highlights its unique attributes and its application in efficiently isolating anomalies within complex data sets."
  },
  {
    "objectID": "prob_and_randvar.html",
    "href": "prob_and_randvar.html",
    "title": "Probability Theory and Random Variables",
    "section": "",
    "text": "Probability theory and random variables are fundamental to machine learning, providing the groundwork for comprehending uncertainty, making predictions, and constructing models that learn from data. In this blog post, we will explore these concepts using R, offering explanations along with some code to show some common examples to enhance understanding."
  },
  {
    "objectID": "prob_and_randvar.html#probability-theory",
    "href": "prob_and_randvar.html#probability-theory",
    "title": "Probability Theory and Random Variables",
    "section": "Probability Theory",
    "text": "Probability Theory\n\nSample Space (S)\nA sample space is the set of all possible outcomes of a random experiment. Let us consider the example of tossing out a fair coin.\n\ns_coin &lt;- c('Heads', 'Tails')\ns_coin\n\n[1] \"Heads\" \"Tails\"\n\n\n\n\nEvent (E)\nAn event is a subset of the sample space, representing a collection of outcomes. For the coin example:\n\ne_head &lt;- c('Heads')\nprint(e_head)\n\n[1] \"Heads\"\n\n\n\n\nProbability (P)\nThe probability of an event is a number between 0 and 1, indicating the likelihood of the event occurring. For a fair coin:\n\np_head &lt;- 0.5\nprint(p_head)\n\n[1] 0.5\n\n\n\n\nProbability Rules\nAddition Rule\nThe addition rule calculates the probability of the union of two events:\n\\[\nP(A \\ \\cup \\ B) = P(A) + P(B) - P(A \\ \\cap \\ B)\n\\]\n\n\nConditional Probability\nConditional probability represents the likelihood of an event given that another event has occurred:\n\\[\nP(A|B) = \\frac{P(A \\ \\cap \\ B)} {P(B)}\n\\]\n\nRandom Variables\nA random variable is a variable whose possible values are numerical outcomes of a random experiment. Consider rolling a fair six-sided die:\n\n# Simulate rolling a die\ndie &lt;- sample(1:6, size=1, replace=TRUE)\n\n\n\n\nDiscrete and Continuous Random Variables\nDiscrete Random Variable\nA discrete random variable takes on a countable number of distinct values. An example is the number of heads in three coin tosses.\nContinuous Random Variable\nA continuous random variable can take on any value in a given range, like the height of a person.\nKeeping these in mind, we are going to use a commonly used example of Coin Tossing\n\nCoin Toss Simulation\n\n## simulate coin tosses\nnum_tosses &lt;- 1000\ntoss_results &lt;- sample(c('Heads', 'Tails'), size=num_tosses, replace = TRUE)\n\n## count occurrences\nheads &lt;- sum(toss_results == 'Heads')\ntails &lt;- num_tosses - heads\n\n## plot results\nbarplot(c(heads, tails), names.arg=c('Heads', 'Tails'), col=c('blue', 'green'), \n        main='Results of Coin Toss Simulation', xlab='Outcomes', ylab='Count')\n\n\n\n\nThis visualization provides a representation of the outcomes of 1000 coin tosses.\n\n\nProbaility Density Function (PDF)\n\n## generate data for a normal distribution \ndata &lt;- rnorm(1000)\n\n## plot the probability density function \n\nhist(data, probability = TRUE, col = 'skyblue', \n     main='Probability Density Function (PDF) of a Normal Distribution', \n     xlab = 'Value', ylab='Probability')\n\n\n\n\nThe histogram above visualizes the probability density function of a normal distribution, highlighting the concentration of values around the mean.\nUnderstanding probability theory and random variables is crucial for building a solid foundation in machine learning. These concepts enable data scientists and machine learning engineers to make informed decision, build accurate models, and navigate the uncertainties inherent in real-world data."
  },
  {
    "objectID": "regression.html",
    "href": "regression.html",
    "title": "Regression: Linear and Non-Linear",
    "section": "",
    "text": "Machine Learning is a realm filled with diverse algorithms and offers powerful tools for understanding relationships between variables and making predictions. Two fundamental tchniques for regression tasks are linear regression and non-linear regression. In this blog, we will embark on a journry to explore concepts, applications, and implementations of both linear and non-linear regression.\n\nLinear Regression\nLinear regression is a straightforward yet powerful method for modeling the relationship between a dependent variable (\\(y\\)) and one or more independent variables (\\(x\\)). The relationship is assumed to be linear, meaning that a change in the independent variable(s) results in a proportional change in the dependent variable.\n\nSimple Linear Regression (SLR)\nIn SLR, there is only one independent variable. The relationship can be expressed as:\n\\[y = \\beta_0 + \\beta_1 . x + \\epsilon\\]\nWhere:\n\n\n\\(y\\) is the dependent variable\n\n\\(x\\) is the independent variable\n\n\\(\\beta_0\\) is the intercept\n\n\\(\\beta_1\\) is the slope\n\n\\(\\epsilon\\) is the error term\n\n\n\n\nMultiple Linear Regression\nWhen there are multiple independent variables, the equation becomes:\n\\[y = \\beta_0 + \\beta_1 . x_1 + \\beta_2 . x_2 + ... + \\beta_n . x_n + \\epsilon\\]\n\n\n\nNon-Linear Regression\nNon-linear regression allows for more complex relationships between variables by using non-linear functions to model the data. This is particularly useful when the linear assumptions of relationships breaks down.\n\nPolynomial Regression\nNon-linear regression is a form of non-linear regression where the relationship between variables is modeled as an n-th degree polynomial. For a quadratic polynomial:\n\\[y = \\beta_0 + \\beta_1.x + \\beta_2. x^2 + \\epsilon\\] #### Exponential and Logarithmic Regression\nWhen the relationship between variables follows an exponential or logarithmic pattern, non-linear regression can capture these relationships effectively.\nExponential Regression: \\(y = \\beta_0 . e^{\\beta_1.x} +\\epsilon\\)\nLogarithmic Regression: \\(y = \\beta_0 + \\beta_1 . ln(x) + \\epsilon\\)\n\n\n\nApplications and Visualiations\n\nReal-world Applications\nLinear and non-linear regression find applications in various applications:\n\n\nEconomics: predicting economic indicators\n\nBiology: modeling growth rates of organisms\n\nFinance: forecasting stock prices  \nEngineering: predicting equipment performance\n\n\n\n\n\nModel Evaluation\n\nAssessing Model Performance\n\nMean Squared Error (MSE): measures the average squared difference between predicted and actual values\n\nR-squared (\\(R^2\\)): indicates the proportion of the variance in the dependen variable that is predictable from the independent variable(s)\n\n\n\n\nApplying Regression\nNext, we are going to apply some linear and non-linear algorithms on a dataset - Advertising Dataset.\n\n\n\nAttaching package: 'olsrr'\n\n\nThe following object is masked from 'package:datasets':\n\n    rivers\n\n\nLoading required package: carData\n\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.3     ✔ readr     2.1.4\n✔ forcats   1.0.0     ✔ stringr   1.5.0\n✔ ggplot2   3.4.4     ✔ tibble    3.2.1\n✔ lubridate 1.9.3     ✔ tidyr     1.3.0\n✔ purrr     1.0.2     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\n✖ dplyr::recode() masks car::recode()\n✖ purrr::some()   masks car::some()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\nLoading required package: lattice\n\n\nAttaching package: 'caret'\n\n\nThe following object is masked from 'package:purrr':\n\n    lift\n\n\n\nhead(data)\n\n     TV Radio Newspaper Sales\n1 230.1  37.8      69.2  22.1\n2  44.5  39.3      45.1  10.4\n3  17.2  45.9      69.3  12.0\n4 151.5  41.3      58.5  16.5\n5 180.8  10.8      58.4  17.9\n6   8.7  48.9      75.0   7.2\n\n\nAn important aspect of regression problems is deciding the target and predictors. This does vary based on the problem at hand or the research questions one is trying to answer. With this data, let’s say we want to use Sales as the Target variable and TV, Radio, Newspaper as Predictors.\nLooking at the scatter plot matrix between each fields in the dataset.\n\npairs(data, upper.panel = NULL)\n\n\n\n\nScatter plots help get sense of correlation between field, which is an important metric for regression. We can see that that TV and Radio, and TV and Newspaper have low correlations. We can also observe that there is a high correlation between TV and Sales, Radio and Sales and Newspaper and Sales.\n\nSplitting Dataset and Modeling\nNext, we split that dataset into train and test splits. We use the training set first to fit the model and then use the testing set to check the performance of the obtained model. We will randomly split the data here.\n\nset.seed(123)\n\ntraining_samples &lt;- data$Sales %&gt;% createDataPartition(p = 0.75, list = FALSE)\n\ntrain &lt;- data[training_samples, ]\ntest &lt;- data[-training_samples, ]\n\n\n\nFitting Simple Linear Regression\nThree different models using the three predictors and we will look at the Adjusted R2 to find the percentage variance explained by these models.\n\nslr1 &lt;- lm(Sales ~ TV, data=train)\n\nsummary(slr1)\n\n\nCall:\nlm(formula = Sales ~ TV, data = train)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-6.4110 -1.7007  0.2769  1.6356  5.7615 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) 6.820068   0.391697   17.41   &lt;2e-16 ***\nTV          0.055899   0.002285   24.46   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 2.391 on 149 degrees of freedom\nMultiple R-squared:  0.8006,    Adjusted R-squared:  0.7993 \nF-statistic: 598.4 on 1 and 149 DF,  p-value: &lt; 2.2e-16\n\n\n\nslr2 &lt;- lm(Sales ~ Radio, data=train)\n\nsummary(slr2)\n\n\nCall:\nlm(formula = Sales ~ Radio, data = train)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-15.9359  -3.5593   0.6252   4.2129   8.1901 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) 12.01533    0.72633  16.542  &lt; 2e-16 ***\nRadio        0.13941    0.02704   5.156 7.91e-07 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 4.932 on 149 degrees of freedom\nMultiple R-squared:  0.1514,    Adjusted R-squared:  0.1457 \nF-statistic: 26.58 on 1 and 149 DF,  p-value: 7.907e-07\n\n\n\nslr3 &lt;- lm(Sales ~ Newspaper, data=train)\n\nsummary(slr3)\n\n\nCall:\nlm(formula = Sales ~ Newspaper, data = train)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-12.5497  -3.8956   0.6684   3.7927  11.4179 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) 13.75564    0.73181  18.797   &lt;2e-16 ***\nNewspaper    0.04530    0.01947   2.326   0.0214 *  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 5.26 on 149 degrees of freedom\nMultiple R-squared:  0.03504,   Adjusted R-squared:  0.02857 \nF-statistic: 5.411 on 1 and 149 DF,  p-value: 0.02136\n\n\nThe first model, with TV as the predictor, explains approximately 80% of variablity of the target with a residual standard error of 2.39. The second model, with Radio as the predictor, explains 15% of variability of the target with an error of 4.93%. The third model, with Newspaper as the predictor, explains 3% of variability of the target with an error of 5.26%.\nClearly, TV as a predictor is explaining more variability of target. However, using a single plot means we might completely neglec the effect of the other two predictors on Sales.\n\n\nFitting Multiple Linear Regression\nThere are multiple methods to extend a simple linear regression. But since, we have only three predictors to work with, we will use a Forward Selection Method to explore more models.\nSince, TV as a predictor explained most variability(80%) earlier, we can add Radio as the two predictors.\n\nmlr1 &lt;- lm(Sales ~ TV + Radio, data=train)\n\nsummary(mlr1)\n\n\nCall:\nlm(formula = Sales ~ TV + Radio, data = train)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-7.4365 -0.8185 -0.0393  0.8595  3.7890 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) 4.585300   0.341251   13.44   &lt;2e-16 ***\nTV          0.054150   0.001658   32.66   &lt;2e-16 ***\nRadio       0.111447   0.009508   11.72   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 1.727 on 148 degrees of freedom\nMultiple R-squared:  0.8966,    Adjusted R-squared:  0.8952 \nF-statistic: 641.8 on 2 and 148 DF,  p-value: &lt; 2.2e-16\n\n\nWe can notice that the Adjusted R2 has improved to 89%. Having said that, we need to check that the imporovement is statistically significant. For this, we will use Analysis of Variance (ANOVA) to test the null hypothesis - H0: The improvement is not significantly significant and H1 - The improvement is statisitically significant.\n\nanova(slr1, mlr1)\n\nAnalysis of Variance Table\n\nModel 1: Sales ~ TV\nModel 2: Sales ~ TV + Radio\n  Res.Df    RSS Df Sum of Sq      F    Pr(&gt;F)    \n1    149 851.61                                  \n2    148 441.63  1    409.98 137.39 &lt; 2.2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nThe p-value is significantly less than 0.05, and therefore we have sufficient evidence to reject the null hypothesis and accept the alternative.\nWe can try to extend the model further with using Newspaper as a third predictor.\n\nmlr2 &lt;- lm(Sales ~ TV + Radio + Newspaper, data=train)\n\nsummary(mlr2)\n\n\nCall:\nlm(formula = Sales ~ TV + Radio + Newspaper, data = train)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-7.4167 -0.8112 -0.0623  0.8476  3.7868 \n\nCoefficients:\n             Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) 4.5730056  0.3618831  12.637   &lt;2e-16 ***\nTV          0.0541379  0.0016672  32.472   &lt;2e-16 ***\nRadio       0.1110996  0.0100967  11.004   &lt;2e-16 ***\nNewspaper   0.0007159  0.0068211   0.105    0.917    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 1.733 on 147 degrees of freedom\nMultiple R-squared:  0.8966,    Adjusted R-squared:  0.8945 \nF-statistic:   425 on 3 and 147 DF,  p-value: &lt; 2.2e-16\n\n\nLooking at the p-value for Newspaper as a predictor, it deems not statistically significant (0.917 &gt; 0.05). Adjusted R2 changed from 89.52 to 89.45. For these reasons, we have evidence to not include Newspaper as a predictor in the model.\n\n\nFitting Orthogonal Polynomial Regression\nAll the assumptions of Multiple Linear Regression were checked and the conclusion was that variance is constant. One last possibility is to check if there is any non-linear relationship between the target and predictors. From the scatter plot, we looked at the beginning, we have noticed that there is some type of curvilinear relationship. For this, we will be using a Polynomial Regression. We will look at specifically Orthogonal Polynomial Regression between Sales and predictors TV and Radio. We already know that Newspaper is not a statistically significant predictor, so we won’t be using Newspaper as a predictor for the non-linear regression.\n\nopr1 &lt;- lm(Sales ~ poly(TV, 2) + poly(Radio, 2) + TV:Radio, data=train)\n\nsummary(opr1)\n\n\nCall:\nlm(formula = Sales ~ poly(TV, 2) + poly(Radio, 2) + TV:Radio, \n    data = train)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-4.8681 -0.7959 -0.0245  0.7578  3.3478 \n\nCoefficients:\n                  Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)      1.354e+01  3.358e-01  40.307  &lt; 2e-16 ***\npoly(TV, 2)1     4.484e+01  2.663e+00  16.835  &lt; 2e-16 ***\npoly(TV, 2)2    -9.000e+00  1.414e+00  -6.365 2.41e-09 ***\npoly(Radio, 2)1  8.346e+00  2.937e+00   2.841  0.00514 ** \npoly(Radio, 2)2  3.440e+00  1.427e+00   2.410  0.01719 *  \nTV:Radio         4.649e-04  9.167e-05   5.071 1.19e-06 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 1.404 on 145 degrees of freedom\nMultiple R-squared:  0.9331,    Adjusted R-squared:  0.9308 \nF-statistic: 404.4 on 5 and 145 DF,  p-value: &lt; 2.2e-16\n\n\nAs seen in the last line of output, this model is statistically significant since p-value &lt;&lt;&lt; 0.5. All the coefficients are statistically significant since all p-value are &lt;&lt;&lt; 0.5. This second order orthogonal polynomial model explains 93.08% variability of target that is a better indication with respect to the multiple linear regression model with TV and Radio as predictor. The Residual standard error for the model is 1.404.\n\nanova(mlr1, opr1)\n\nAnalysis of Variance Table\n\nModel 1: Sales ~ TV + Radio\nModel 2: Sales ~ poly(TV, 2) + poly(Radio, 2) + TV:Radio\n  Res.Df    RSS Df Sum of Sq     F    Pr(&gt;F)    \n1    148 441.63                                 \n2    145 285.85  3    155.78 26.34 1.168e-13 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nSince the p-value for testing the null hypothesis (\\(1.168e-13\\)) is significantly less than 0.05, hence we have sufficient evidence to reject the null hypothesis and accept the alternative. Therefore, the improvement in the Adjusteed R2 is statistically significant. To have bases covered, we will also try fitting a third order orthogonal polynomial regression.\n\nopr2 &lt;- lm(Sales ~ poly(TV,3) + poly(Radio,3) + TV:Radio, data=train)\nsummary(opr2)\n\n\nCall:\nlm(formula = Sales ~ poly(TV, 3) + poly(Radio, 3) + TV:Radio, \n    data = train)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-4.5998 -0.6947  0.0119  0.6711  3.2824 \n\nCoefficients:\n                  Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)      1.359e+01  3.300e-01  41.183  &lt; 2e-16 ***\npoly(TV, 3)1     4.481e+01  2.613e+00  17.148  &lt; 2e-16 ***\npoly(TV, 3)2    -8.288e+00  1.401e+00  -5.915 2.34e-08 ***\npoly(TV, 3)3     9.420e-01  1.393e+00   0.676  0.50011    \npoly(Radio, 3)1  8.850e+00  2.907e+00   3.044  0.00278 ** \npoly(Radio, 3)2  3.470e+00  1.392e+00   2.493  0.01380 *  \npoly(Radio, 3)3  4.148e+00  1.401e+00   2.962  0.00359 ** \nTV:Radio         4.494e-04  9.016e-05   4.984 1.78e-06 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 1.369 on 143 degrees of freedom\nMultiple R-squared:  0.9372,    Adjusted R-squared:  0.9342 \nF-statistic: 305.1 on 7 and 143 DF,  p-value: &lt; 2.2e-16\n\n\nIt is clear from the results above that third order of TV predictor is not statistically significant (p-value &gt; 0.05). Hence, we won’t include this in the model. Hence, the fit is -\n\nopr3 &lt;- lm(Sales ~ poly(TV,2) + poly(Radio,3) + TV:Radio, data=train)\nsummary(opr3)\n\n\nCall:\nlm(formula = Sales ~ poly(TV, 2) + poly(Radio, 3) + TV:Radio, \n    data = train)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-4.7550 -0.6920 -0.0132  0.7186  3.2248 \n\nCoefficients:\n                  Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)      1.356e+01  3.270e-01  41.478  &lt; 2e-16 ***\npoly(TV, 2)1     4.462e+01  2.593e+00  17.206  &lt; 2e-16 ***\npoly(TV, 2)2    -8.262e+00  1.398e+00  -5.910 2.37e-08 ***\npoly(Radio, 3)1  8.518e+00  2.860e+00   2.979  0.00340 ** \npoly(Radio, 3)2  3.473e+00  1.389e+00   2.500  0.01353 *  \npoly(Radio, 3)3  4.201e+00  1.396e+00   3.010  0.00308 ** \nTV:Radio         4.571e-04  8.926e-05   5.121 9.58e-07 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 1.367 on 144 degrees of freedom\nMultiple R-squared:  0.937, Adjusted R-squared:  0.9344 \nF-statistic: 357.2 on 6 and 144 DF,  p-value: &lt; 2.2e-16\n\n\nChecking all the assumptions of regression.\n\n# Linearity Assumption\nplot(opr3 ,1)\n\n\n\n# Homoscedasticity Assumption \nols_test_score(opr3)\n\n\n Score Test for Heteroskedasticity\n ---------------------------------\n Ho: Variance is homogenous\n Ha: Variance is not homogenous\n\n Variables: fitted values of Sales \n\n        Test Summary         \n ----------------------------\n DF            =    1 \n Chi2          =    1.497247 \n Prob &gt; Chi2   =    0.2210954 \n\n# Autocorrelation Assumption \ndurbinWatsonTest(opr3)\n\n lag Autocorrelation D-W Statistic p-value\n   1      -0.0442372      2.076031    0.57\n Alternative hypothesis: rho != 0\n\n# Normality Assumption\nshapiro.test(opr3$residuals)\n\n\n    Shapiro-Wilk normality test\n\ndata:  opr3$residuals\nW = 0.98162, p-value = 0.04121\n\n# Multicolinearity Assumption\nvif(opr3)\n\nthere are higher-order terms (interactions) in this model\nconsider setting type = 'predictor'; see ?vif\n\n\n                   GVIF Df GVIF^(1/(2*Df))\npoly(TV, 2)    3.764510  2        1.392923\npoly(Radio, 3) 4.609044  3        1.290036\nTV:Radio       7.526778  1        2.743497\n\n\n\n\nMaking Predictions\n\nprediction &lt;- opr3 %&gt;% predict(test)\n\n## Cheking performance by calculating R2, RMSE, and MAE\n\ndata.frame( R2 = R2(prediction, test$Sales), \n            RMSE = RMSE(prediction, test$Sales),\n            MAE = MAE(prediction, test$Sales))\n\n         R2     RMSE       MAE\n1 0.9395374 1.264048 0.9417405\n\n\nThe R2 we obtain seems like a decent fit, however this result is based on one test data set. Hence, we cannot be certain that the model will perform better on unseen data. Therefore, we will use K-fold cross validation to test the performance of model on a different test data set.\n\nset.seed(123)\n\ntrain_control &lt;- trainControl(method = \"repeatedcv\", number = 10, repeats = 3)\n\nmodel_cv &lt;- train(Sales ~ poly(TV,2) + poly(Radio,3) + TV:Radio, data=data, method = \"lm\",                    trControl = train_control)\n\nprint(model_cv)\n\nLinear Regression \n\n199 samples\n  2 predictor\n\nNo pre-processing\nResampling: Cross-Validated (10 fold, repeated 3 times) \nSummary of sample sizes: 179, 179, 179, 179, 180, 178, ... \nResampling results:\n\n  RMSE      Rsquared   MAE      \n  1.298474  0.9392671  0.9752946\n\nTuning parameter 'intercept' was held constant at a value of TRUE\n\n\nThis model captures almost 94% variability available in the target on average. Therefore, we can say that this is still a decent fit.\nTo summarize, we looked at Simple Linear Regression and expanded those to Multiple Linear Regression to find a better fit. To cover all bases, we also tried to fit a polynomial regression model. Ultimately, we found a better fit by comparing variability and p-values. In conclusion, linear and non-linear regression stand as pillars for modeling relationships and making predictions. Understanding when to choose one over the other and how to interpret the results is essential."
  },
  {
    "objectID": "clustering.html",
    "href": "clustering.html",
    "title": "Clustering: Unraveling Patterns",
    "section": "",
    "text": "Clustering involves grouping similar data points together, revealing inherent structures within data sets. In this blog post, we will dive into the realm of clustering, exploring its concepts, applications, and implementations in R. To enhance understanding, we will include some code snippets and visualizations using the Mall Customers data set, offering an interesting perspective on the power of clustering.\nhead(customer)\n\n  CustomerID Gender Age Annual.Income..k.. Spending.Score..1.100.\n1          1   Male  19                 15                     39\n2          2   Male  21                 15                     81\n3          3 Female  20                 16                      6\n4          4 Female  23                 16                     77\n5          5 Female  31                 17                     40\n6          6 Female  22                 17                     76"
  },
  {
    "objectID": "clustering.html#k-means-clustering",
    "href": "clustering.html#k-means-clustering",
    "title": "Clustering: Unraveling Patterns",
    "section": "K-Means Clustering",
    "text": "K-Means Clustering\nK-Means is a popular partitioning method where the algorithm divides the data set into “k” clusters, aiming to minimize the within-cluster sum of squares."
  },
  {
    "objectID": "clustering.html#hierarchical-clustering",
    "href": "clustering.html#hierarchical-clustering",
    "title": "Clustering: Unraveling Patterns",
    "section": "Hierarchical Clustering",
    "text": "Hierarchical Clustering\nHierarchical clustering builds a tree-like structure of clusters, allowing for a visual representation of the relationships between data points.\n\nImplementing Clustering for Segmentation"
  },
  {
    "objectID": "clustering.html#data-preparation",
    "href": "clustering.html#data-preparation",
    "title": "Clustering: Unraveling Patterns",
    "section": "Data Preparation",
    "text": "Data Preparation\n\ncustomer &lt;- customer[,4:5]\nplot(customer)"
  },
  {
    "objectID": "clustering.html#applying-k-means",
    "href": "clustering.html#applying-k-means",
    "title": "Clustering: Unraveling Patterns",
    "section": "Applying K-Means",
    "text": "Applying K-Means\nBased on the scatter plot above, and to apply basic K-means algorithm, we use number of clusters = 3.\n\n## components of k-means\nkmeans(customer, 3)\n\nK-means clustering with 3 clusters of sizes 38, 39, 123\n\nCluster means:\n  Annual.Income..k.. Spending.Score..1.100.\n1           87.00000               18.63158\n2           86.53846               82.12821\n3           44.15447               49.82927\n\nClustering vector:\n  [1] 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3\n [38] 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3\n [75] 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3\n[112] 3 3 3 3 3 3 3 3 3 3 3 3 2 1 2 1 2 1 2 1 2 1 2 1 2 1 2 1 2 1 2 1 2 1 2 1 2\n[149] 1 2 1 2 1 2 1 2 1 2 1 2 1 2 1 2 1 2 1 2 1 2 1 2 1 2 1 2 1 2 1 2 1 2 1 2 1\n[186] 2 1 2 1 2 1 2 1 2 1 2 1 2 1 2\n\nWithin cluster sum of squares by cluster:\n[1] 14204.84 13444.05 78699.48\n (between_SS / total_SS =  60.6 %)\n\nAvailable components:\n\n[1] \"cluster\"      \"centers\"      \"totss\"        \"withinss\"     \"tot.withinss\"\n[6] \"betweenss\"    \"size\"         \"iter\"         \"ifault\""
  },
  {
    "objectID": "clustering.html#elbow-method",
    "href": "clustering.html#elbow-method",
    "title": "Clustering: Unraveling Patterns",
    "section": "Elbow Method",
    "text": "Elbow Method\nInstead intuitively picking the number of factors, we can use code to assess the within-cluster sum of squares (WCSS) for different values of K and look for the “elbow” point where the rate decrease in WCSS starts to decrease.\n\nset.seed(123)\nsum_of_squares &lt;- vector()\n\nfor (i in 1:10) {\n  x &lt;- (sum(kmeans(customer, i)$withinss))\n  sum_of_squares[i] = x\n  print(x)\n}\n\n[1] 269981.3\n[1] 186206.8\n[1] 106348.4\n[1] 73679.79\n[1] 66733.44\n[1] 37233.81\n[1] 33610.53\n[1] 29420.64\n[1] 28378.97\n[1] 25798.02\n\n\n\nplot(x=1:10, y=sum_of_squares, type='o')\n\n\n\n\nWe can see that the sum of squares has a drastic drop at 6 clusters.\n\n## fir the model with k=6\nmodel_k6 &lt;- kmeans(customer, 6)\nmodel_k6\n\nK-means clustering with 6 clusters of sizes 22, 12, 15, 35, 39, 77\n\nCluster means:\n  Annual.Income..k.. Spending.Score..1.100.\n1           25.72727              79.363636\n2           24.58333               9.583333\n3           31.53333              35.866667\n4           88.20000              17.114286\n5           86.53846              82.128205\n6           56.05195              49.857143\n\nClustering vector:\n  [1] 3 1 2 1 3 1 2 1 2 1 2 1 2 1 2 1 3 1 3 1 3 1 2 1 2 1 3 1 3 1 2 1 2 1 2 1 2\n [38] 1 3 1 3 1 3 6 3 1 6 3 3 3 6 6 6 6 6 3 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6\n [75] 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6\n[112] 6 6 6 6 6 6 6 6 6 6 6 6 5 4 5 6 5 4 5 4 5 6 5 4 5 4 5 4 5 4 5 6 5 4 5 4 5\n[149] 4 5 4 5 4 5 4 5 4 5 4 5 4 5 4 5 4 5 4 5 4 5 4 5 4 5 4 5 4 5 4 5 4 5 4 5 4\n[186] 5 4 5 4 5 4 5 4 5 4 5 4 5 4 5\n\nWithin cluster sum of squares by cluster:\n[1]  3519.4545   799.8333  1773.4667 12511.1429 13444.0513  8777.2208\n (between_SS / total_SS =  84.9 %)\n\nAvailable components:\n\n[1] \"cluster\"      \"centers\"      \"totss\"        \"withinss\"     \"tot.withinss\"\n[6] \"betweenss\"    \"size\"         \"iter\"         \"ifault\"      \n\n\n\n## visualizing the results for k=6\nlibrary(cluster)\n\nclusplot(customer, model_k6$cluster, lines=0, shade=T, color=T)"
  },
  {
    "objectID": "clustering.html#applying-hierarchical-clustering",
    "href": "clustering.html#applying-hierarchical-clustering",
    "title": "Clustering: Unraveling Patterns",
    "section": "Applying Hierarchical Clustering",
    "text": "Applying Hierarchical Clustering\nIn hierarchical clustering, we will use euclidean distance to learn distance between the data points.\n\nd &lt;- dist(customer, method = 'euclidean')\n\n\nlibrary(repr)\noptions(repr.plot.width=15, repr.plot.height = 12)\n\n## creating a dendogram to visualize the hierarchy\n\ndendogram &lt;- hclust(d, method = 'ward.D')\nplot(dendogram)\nabline(h=700,col='red')\n\n\n\n\nRed line in the diagram above, cut the 5 hierarchies that have high euclidean distance.\n\nhier &lt;- hclust(d, method = 'ward.D')\ny_hier &lt;- cutree(hier, 5)\ny_hier\n\n  [1] 1 2 1 2 1 2 1 2 1 2 1 2 1 2 1 2 1 2 1 2 1 2 1 2 1 2 1 2 1 2 1 2 1 2 1 2 1\n [38] 2 1 2 1 2 1 3 1 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3\n [75] 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3\n[112] 3 3 3 3 3 3 3 3 3 3 3 3 4 3 4 3 4 5 4 5 4 3 4 5 4 5 4 5 4 5 4 3 4 5 4 3 4\n[149] 5 4 5 4 5 4 5 4 5 4 5 4 3 4 5 4 5 4 5 4 5 4 5 4 5 4 5 4 5 4 5 4 5 4 5 4 5\n[186] 4 5 4 5 4 5 4 5 4 5 4 5 4 5 4\n\n\n\nclusplot(customer, clus=y_hier, lines=0, color=TRUE)\n\n\n\n\nThe plot above offer a visual representation of clusters for the data set.\nClustering is a versatile technique that uncovers patterns and structures within data sets. By implementing clustering algorithms such as K-Means and Hierarchical Clustering. We were able to gain valuable insights into the inherent relationships among data points. The visualizations not only aid in understanding the results but also serve as powerful tools for conveying complex patterns."
  },
  {
    "objectID": "classification.html",
    "href": "classification.html",
    "title": "Classification: Unveiling Insights",
    "section": "",
    "text": "Machine learning, a dynamic field at the intersection of computer science and statistics, is empowered by diverse algorithms designed to make sense of data. One of the foundational tasks within machine learning is classification - a process that assigns predefined labels or categories to input data. In this blog post, we will embark on a journey to unravel the intricacies of classification in machine learning (using R), exploring its significance, techniques, and applications.\n\nUnderstanding Classification\nClassification is a supervised learning technique where the algorithm learns from labeled training data and predicts the labels of unseen or new data points. In essence, it is the process of mapping input data to predefined classes.\n\n\nImportance of Classification\nClassification plays a pivotal role in a myriad of applications in this day and age:\n\nSpam detection: Classifying emails as spam or not spam\nMedical Diagnosis: Identifying diseases based on patient data\nCredit Scoring: Assessing the creditworthiness of individuals\nImage Recognition: Recognizing objects or patterns in images\nSentiment Analysis: Analyzing text to determine sentiment (positive, negative, neutral)\n\n\n\nTechniques in Classification\n\nLogistic Regression\nDespite the name, logistic regression is a classification algorithm used for binary classification problems. It models the probability of an instance belonging to a particular class.\n\n\nDecision Trees\nDecision trees are tree-like structures where each node represents a decision based on a feature. They recursively split the dataset to create a tree that predicts the class label.\n\n\nSupport Vector Machines (SVM)\nSVM is a powerful algorithm for both binary and multiclass classification. It aims to find the hyperplane that best separates different classes in feature space.\n\n\nRandom Forest\nRandom Forest is an ensemble learning method that constructs multiple decision trees during training and outputs the class that is the mode of the classes (classification) or mean prediction (regression) of the individual trees.\n\n\n\nEvaluating Classification Models\n\nConfusion Matrix\nA confusion matrix is a table that describes the performance of a classification model. It summarizes the number of true positive, true negative, false positive, and false negative predictions.\n\n\nAccuracy, Precision, Recall, F1 Score\nThese metrics provide a more detailed assessment of a classification model’s performance:\n\nAccuracy: Proportion of correctly classified instances.\nPrecision: Proportion of true positive predictions among all positive predictions.\nRecall (Sensitivity): Proportion of true positive predictions among all actual positive instances.\nF1 Score: A balance between precision and recall.\n\nIn this blog, we will be using a `Wine Quality’ dataset to apply some classification techniques, mainly Logistic Regression and Decision Trees.\n\n\nType 'citation(\"pROC\")' for a citation.\n\n\n\nAttaching package: 'pROC'\n\n\nThe following objects are masked from 'package:stats':\n\n    cov, smooth, var\n\n\nLoading required package: ggplot2\n\n\nLoading required package: lattice\n\n\n\nhead(wine)\n\n  fixed.acidity volatile.acidity citric.acid residual.sugar chlorides\n1           7.4             0.70        0.00            1.9     0.076\n2           7.8             0.88        0.00            2.6     0.098\n3           7.8             0.76        0.04            2.3     0.092\n4          11.2             0.28        0.56            1.9     0.075\n5           7.4             0.70        0.00            1.9     0.076\n6           7.4             0.66        0.00            1.8     0.075\n  free.sulfur.dioxide total.sulfur.dioxide density   pH sulphates alcohol\n1                  11                   34  0.9978 3.51      0.56     9.4\n2                  25                   67  0.9968 3.20      0.68     9.8\n3                  15                   54  0.9970 3.26      0.65     9.8\n4                  17                   60  0.9980 3.16      0.58     9.8\n5                  11                   34  0.9978 3.51      0.56     9.4\n6                  13                   40  0.9978 3.51      0.56     9.4\n  quality\n1       5\n2       5\n3       5\n4       6\n5       5\n6       5\n\n\nIn this data set the first 11 are the objective variables that measure different chemical aspects of different wines. The 12th dimension quality is the output variable, the one we are trying to predict.\nTo make the classification simpler, we are going to put the quality column into categories. A quality score of 3 or 4 will be classified as Low, 5 or 6 as Medium and & or 8 as High.\n\nwine$low &lt;- wine$quality &lt;= 4\nwine$medium &lt;- wine$quality == 5 | wine$quality == 6\nwine$high &lt;- wine$quality &gt;= 7\nhead(wine)\n\n  fixed.acidity volatile.acidity citric.acid residual.sugar chlorides\n1           7.4             0.70        0.00            1.9     0.076\n2           7.8             0.88        0.00            2.6     0.098\n3           7.8             0.76        0.04            2.3     0.092\n4          11.2             0.28        0.56            1.9     0.075\n5           7.4             0.70        0.00            1.9     0.076\n6           7.4             0.66        0.00            1.8     0.075\n  free.sulfur.dioxide total.sulfur.dioxide density   pH sulphates alcohol\n1                  11                   34  0.9978 3.51      0.56     9.4\n2                  25                   67  0.9968 3.20      0.68     9.8\n3                  15                   54  0.9970 3.26      0.65     9.8\n4                  17                   60  0.9980 3.16      0.58     9.8\n5                  11                   34  0.9978 3.51      0.56     9.4\n6                  13                   40  0.9978 3.51      0.56     9.4\n  quality   low medium  high\n1       5 FALSE   TRUE FALSE\n2       5 FALSE   TRUE FALSE\n3       5 FALSE   TRUE FALSE\n4       6 FALSE   TRUE FALSE\n5       5 FALSE   TRUE FALSE\n6       5 FALSE   TRUE FALSE\n\n\n\n\n\nApplying Logisitc Regression\nFor the classification techniques here we will use two approches/models - one using just the alcohol level and one with more than one variable.\nFor the first model, we will narrow out down to just being interested in High quality wines.\n\nlog1_high &lt;- glm (high~alcohol, data=wine, family=binomial(link='logit'))\nlog2_high &lt;- glm(high~alcohol + volatile.acidity + citric.acid + sulphates, data=wine, family=binomial(link='logit'))\n\nsummary(log1_high)\n\n\nCall:\nglm(formula = high ~ alcohol, family = binomial(link = \"logit\"), \n    data = wine)\n\nCoefficients:\n             Estimate Std. Error z value Pr(&gt;|z|)    \n(Intercept) -13.23466    0.84022  -15.75   &lt;2e-16 ***\nalcohol       1.05057    0.07486   14.03   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for binomial family taken to be 1)\n\n    Null deviance: 1269.9  on 1598  degrees of freedom\nResidual deviance: 1027.9  on 1597  degrees of freedom\nAIC: 1031.9\n\nNumber of Fisher Scoring iterations: 5\n\nsummary(log2_high)\n\n\nCall:\nglm(formula = high ~ alcohol + volatile.acidity + citric.acid + \n    sulphates, family = binomial(link = \"logit\"), data = wine)\n\nCoefficients:\n                 Estimate Std. Error z value Pr(&gt;|z|)    \n(Intercept)      -13.1586     1.1252 -11.694  &lt; 2e-16 ***\nalcohol            1.0114     0.0814  12.424  &lt; 2e-16 ***\nvolatile.acidity  -3.5839     0.6854  -5.229 1.70e-07 ***\ncitric.acid        0.9378     0.5270   1.780   0.0751 .  \nsulphates          2.4424     0.4488   5.442 5.27e-08 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for binomial family taken to be 1)\n\n    Null deviance: 1269.92  on 1598  degrees of freedom\nResidual deviance:  914.11  on 1594  degrees of freedom\nAIC: 924.11\n\nNumber of Fisher Scoring iterations: 6\n\n\nLooking at the AIC (Akaike information criterion), which gets lower as the model gets better, we can see that combining variables together prove to be better classifying model than just using the alcohol variable. Next, we will make some predictions and use some visualizations to assess the two models.\n\np_log1_high &lt;- pnorm(predict(log1_high))\np_log2_high &lt;- pnorm(predict(log2_high))\n\n\nauc1 &lt;- plot.roc(wine$high, p_log1_high, main=\"\", percent=TRUE, ci=TRUE, print.auc=TRUE)\n\nSetting levels: control = FALSE, case = TRUE\n\n\nSetting direction: controls &lt; cases\n\nauc1.se &lt;- ci.se(auc1, specificities=seq(0,100,5))\nplot(auc1.se,type=\"shape\", col=\"grey\")\n\n\n\nauc2 &lt;- plot.roc(wine$high, p_log2_high, main=\"\", percent=TRUE, ci=TRUE, print.auc=TRUE)\n\nSetting levels: control = FALSE, case = TRUE\nSetting direction: controls &lt; cases\n\nauc2.se &lt;- ci.se(auc2, specificities=seq(0,100,5))\nplot(auc2.se,type=\"shape\", col=\"blue\")\n\n\n\n\nThese visualizations further support our claim that the second model with more variables is better (the higher the AUC the better the model is).\n\n\nApplying Decision Trees\n\ndtree1 &lt;- rpart(high ~ alcohol + sulphates, data=wine, method = \"class\")\nsummary(dtree1)\n\nCall:\nrpart(formula = high ~ alcohol + sulphates, data = wine, method = \"class\")\n  n= 1599 \n\n          CP nsplit rel error    xerror       xstd\n1 0.08294931      0 1.0000000 1.0000000 0.06311025\n2 0.01000000      2 0.8341014 0.8341014 0.05838395\n\nVariable importance\n  alcohol sulphates \n       74        26 \n\nNode number 1: 1599 observations,    complexity param=0.08294931\n  predicted class=FALSE  expected loss=0.1357098  P(node) =1\n    class counts:  1382   217\n   probabilities: 0.864 0.136 \n  left son=2 (1349 obs) right son=3 (250 obs)\n  Primary splits:\n      alcohol   &lt; 11.55 to the left,  improve=54.87599, (0 missing)\n      sulphates &lt; 0.675 to the left,  improve=33.01760, (0 missing)\n\nNode number 2: 1349 observations\n  predicted class=FALSE  expected loss=0.07931801  P(node) =0.8436523\n    class counts:  1242   107\n   probabilities: 0.921 0.079 \n\nNode number 3: 250 observations,    complexity param=0.08294931\n  predicted class=FALSE  expected loss=0.44  P(node) =0.1563477\n    class counts:   140   110\n   probabilities: 0.560 0.440 \n  left son=6 (136 obs) right son=7 (114 obs)\n  Primary splits:\n      sulphates &lt; 0.685 to the left,  improve=19.8989200, (0 missing)\n      alcohol   &lt; 12.05 to the left,  improve= 0.5101698, (0 missing)\n  Surrogate splits:\n      alcohol &lt; 13.25 to the left,  agree=0.592, adj=0.105, (0 split)\n\nNode number 6: 136 observations\n  predicted class=FALSE  expected loss=0.2573529  P(node) =0.08505316\n    class counts:   101    35\n   probabilities: 0.743 0.257 \n\nNode number 7: 114 observations\n  predicted class=TRUE   expected loss=0.3421053  P(node) =0.07129456\n    class counts:    39    75\n   probabilities: 0.342 0.658 \n\n\n\nrpart.plot(dtree1)\n\n\n\npred_dtree1 &lt;- predict(dtree1, newdata=wine, type=\"class\")\n\n\ndtree2 &lt;- rpart(high ~ alcohol + volatile.acidity + citric.acid + sulphates, data=wine, method = \"class\")\nsummary(dtree2)\n\nCall:\nrpart(formula = high ~ alcohol + volatile.acidity + citric.acid + \n    sulphates, data = wine, method = \"class\")\n  n= 1599 \n\n          CP nsplit rel error    xerror       xstd\n1 0.08294931      0 1.0000000 1.0000000 0.06311025\n2 0.01382488      2 0.8341014 0.9078341 0.06056520\n3 0.01152074      8 0.7465438 0.9400922 0.06147779\n4 0.01000000     11 0.7096774 0.9216590 0.06095929\n\nVariable importance\n         alcohol        sulphates volatile.acidity      citric.acid \n              51               20               16               14 \n\nNode number 1: 1599 observations,    complexity param=0.08294931\n  predicted class=FALSE  expected loss=0.1357098  P(node) =1\n    class counts:  1382   217\n   probabilities: 0.864 0.136 \n  left son=2 (1349 obs) right son=3 (250 obs)\n  Primary splits:\n      alcohol          &lt; 11.55 to the left,  improve=54.87599, (0 missing)\n      volatile.acidity &lt; 0.385 to the right, improve=35.03893, (0 missing)\n      sulphates        &lt; 0.675 to the left,  improve=33.01760, (0 missing)\n      citric.acid      &lt; 0.315 to the left,  improve=27.06204, (0 missing)\n  Surrogate splits:\n      volatile.acidity &lt; 0.14  to the right, agree=0.846, adj=0.012, (0 split)\n\nNode number 2: 1349 observations,    complexity param=0.01382488\n  predicted class=FALSE  expected loss=0.07931801  P(node) =0.8436523\n    class counts:  1242   107\n   probabilities: 0.921 0.079 \n  left son=4 (1111 obs) right son=5 (238 obs)\n  Primary splits:\n      volatile.acidity &lt; 0.375 to the right, improve=17.25464, (0 missing)\n      alcohol          &lt; 10.45 to the left,  improve=12.96381, (0 missing)\n      sulphates        &lt; 0.675 to the left,  improve=12.53088, (0 missing)\n      citric.acid      &lt; 0.295 to the left,  improve= 9.43151, (0 missing)\n  Surrogate splits:\n      citric.acid &lt; 0.71  to the left,  agree=0.824, adj=0.004, (0 split)\n\nNode number 3: 250 observations,    complexity param=0.08294931\n  predicted class=FALSE  expected loss=0.44  P(node) =0.1563477\n    class counts:   140   110\n   probabilities: 0.560 0.440 \n  left son=6 (136 obs) right son=7 (114 obs)\n  Primary splits:\n      sulphates        &lt; 0.685 to the left,  improve=19.8989200, (0 missing)\n      citric.acid      &lt; 0.315 to the left,  improve= 9.1723060, (0 missing)\n      volatile.acidity &lt; 0.425 to the right, improve= 6.7370480, (0 missing)\n      alcohol          &lt; 12.05 to the left,  improve= 0.5101698, (0 missing)\n  Surrogate splits:\n      citric.acid      &lt; 0.305 to the left,  agree=0.628, adj=0.184, (0 split)\n      volatile.acidity &lt; 0.345 to the right, agree=0.596, adj=0.114, (0 split)\n      alcohol          &lt; 13.25 to the left,  agree=0.592, adj=0.105, (0 split)\n\nNode number 4: 1111 observations\n  predicted class=FALSE  expected loss=0.04230423  P(node) =0.6948093\n    class counts:  1064    47\n   probabilities: 0.958 0.042 \n\nNode number 5: 238 observations,    complexity param=0.01382488\n  predicted class=FALSE  expected loss=0.2521008  P(node) =0.148843\n    class counts:   178    60\n   probabilities: 0.748 0.252 \n  left son=10 (120 obs) right son=11 (118 obs)\n  Primary splits:\n      alcohol          &lt; 10.45 to the left,  improve=12.459480, (0 missing)\n      sulphates        &lt; 0.635 to the left,  improve= 7.099497, (0 missing)\n      citric.acid      &lt; 0.285 to the left,  improve= 2.817234, (0 missing)\n      volatile.acidity &lt; 0.205 to the left,  improve= 1.465961, (0 missing)\n  Surrogate splits:\n      sulphates        &lt; 0.675 to the left,  agree=0.601, adj=0.195, (0 split)\n      citric.acid      &lt; 0.285 to the left,  agree=0.584, adj=0.161, (0 split)\n      volatile.acidity &lt; 0.345 to the left,  agree=0.542, adj=0.076, (0 split)\n\nNode number 6: 136 observations,    complexity param=0.01152074\n  predicted class=FALSE  expected loss=0.2573529  P(node) =0.08505316\n    class counts:   101    35\n   probabilities: 0.743 0.257 \n  left son=12 (95 obs) right son=13 (41 obs)\n  Primary splits:\n      citric.acid      &lt; 0.445 to the left,  improve=4.984524, (0 missing)\n      volatile.acidity &lt; 0.385 to the right, improve=3.217852, (0 missing)\n      sulphates        &lt; 0.615 to the left,  improve=2.845143, (0 missing)\n      alcohol          &lt; 12.45 to the right, improve=1.220143, (0 missing)\n  Surrogate splits:\n      volatile.acidity &lt; 0.305 to the right, agree=0.728, adj=0.098, (0 split)\n      sulphates        &lt; 0.635 to the left,  agree=0.728, adj=0.098, (0 split)\n      alcohol          &lt; 13.15 to the left,  agree=0.706, adj=0.024, (0 split)\n\nNode number 7: 114 observations\n  predicted class=TRUE   expected loss=0.3421053  P(node) =0.07129456\n    class counts:    39    75\n   probabilities: 0.342 0.658 \n\nNode number 10: 120 observations\n  predicted class=FALSE  expected loss=0.09166667  P(node) =0.0750469\n    class counts:   109    11\n   probabilities: 0.908 0.092 \n\nNode number 11: 118 observations,    complexity param=0.01382488\n  predicted class=FALSE  expected loss=0.4152542  P(node) =0.07379612\n    class counts:    69    49\n   probabilities: 0.585 0.415 \n  left son=22 (58 obs) right son=23 (60 obs)\n  Primary splits:\n      sulphates        &lt; 0.735 to the left,  improve=4.4326710, (0 missing)\n      citric.acid      &lt; 0.335 to the left,  improve=0.8028625, (0 missing)\n      alcohol          &lt; 10.75 to the left,  improve=0.6463546, (0 missing)\n      volatile.acidity &lt; 0.235 to the left,  improve=0.4687211, (0 missing)\n  Surrogate splits:\n      alcohol          &lt; 11.05 to the right, agree=0.619, adj=0.224, (0 split)\n      citric.acid      &lt; 0.425 to the left,  agree=0.610, adj=0.207, (0 split)\n      volatile.acidity &lt; 0.335 to the right, agree=0.585, adj=0.155, (0 split)\n\nNode number 12: 95 observations\n  predicted class=FALSE  expected loss=0.1684211  P(node) =0.05941213\n    class counts:    79    16\n   probabilities: 0.832 0.168 \n\nNode number 13: 41 observations,    complexity param=0.01152074\n  predicted class=FALSE  expected loss=0.4634146  P(node) =0.02564103\n    class counts:    22    19\n   probabilities: 0.537 0.463 \n  left son=26 (12 obs) right son=27 (29 obs)\n  Primary splits:\n      alcohol          &lt; 12.45 to the right, improve=2.987945, (0 missing)\n      citric.acid      &lt; 0.55  to the right, improve=1.664754, (0 missing)\n      sulphates        &lt; 0.58  to the left,  improve=1.195244, (0 missing)\n      volatile.acidity &lt; 0.275 to the left,  improve=0.533101, (0 missing)\n  Surrogate splits:\n      citric.acid &lt; 0.685 to the right, agree=0.756, adj=0.167, (0 split)\n      sulphates   &lt; 0.655 to the right, agree=0.732, adj=0.083, (0 split)\n\nNode number 22: 58 observations\n  predicted class=FALSE  expected loss=0.2758621  P(node) =0.03627267\n    class counts:    42    16\n   probabilities: 0.724 0.276 \n\nNode number 23: 60 observations,    complexity param=0.01382488\n  predicted class=TRUE   expected loss=0.45  P(node) =0.03752345\n    class counts:    27    33\n   probabilities: 0.450 0.550 \n  left son=46 (12 obs) right son=47 (48 obs)\n  Primary splits:\n      citric.acid      &lt; 0.345 to the left,  improve=2.7000000, (0 missing)\n      sulphates        &lt; 0.775 to the right, improve=1.5000000, (0 missing)\n      volatile.acidity &lt; 0.335 to the right, improve=0.5730159, (0 missing)\n      alcohol          &lt; 10.85 to the right, improve=0.3080586, (0 missing)\n\nNode number 26: 12 observations\n  predicted class=FALSE  expected loss=0.1666667  P(node) =0.00750469\n    class counts:    10     2\n   probabilities: 0.833 0.167 \n\nNode number 27: 29 observations,    complexity param=0.01152074\n  predicted class=TRUE   expected loss=0.4137931  P(node) =0.01813634\n    class counts:    12    17\n   probabilities: 0.414 0.586 \n  left son=54 (19 obs) right son=55 (10 obs)\n  Primary splits:\n      alcohol          &lt; 12.05 to the left,  improve=3.005808, (0 missing)\n      sulphates        &lt; 0.575 to the left,  improve=2.617985, (0 missing)\n      citric.acid      &lt; 0.55  to the right, improve=1.755834, (0 missing)\n      volatile.acidity &lt; 0.37  to the left,  improve=1.098377, (0 missing)\n  Surrogate splits:\n      volatile.acidity &lt; 0.315 to the right, agree=0.724, adj=0.2, (0 split)\n      citric.acid      &lt; 0.485 to the right, agree=0.690, adj=0.1, (0 split)\n\nNode number 46: 12 observations\n  predicted class=FALSE  expected loss=0.25  P(node) =0.00750469\n    class counts:     9     3\n   probabilities: 0.750 0.250 \n\nNode number 47: 48 observations,    complexity param=0.01382488\n  predicted class=TRUE   expected loss=0.375  P(node) =0.03001876\n    class counts:    18    30\n   probabilities: 0.375 0.625 \n  left son=94 (26 obs) right son=95 (22 obs)\n  Primary splits:\n      citric.acid      &lt; 0.475 to the right, improve=3.0314690, (0 missing)\n      sulphates        &lt; 0.795 to the left,  improve=0.7500000, (0 missing)\n      volatile.acidity &lt; 0.355 to the left,  improve=0.5170940, (0 missing)\n      alcohol          &lt; 11.05 to the right, improve=0.3947368, (0 missing)\n  Surrogate splits:\n      sulphates        &lt; 0.795 to the left,  agree=0.750, adj=0.455, (0 split)\n      alcohol          &lt; 10.95 to the left,  agree=0.667, adj=0.273, (0 split)\n      volatile.acidity &lt; 0.255 to the right, agree=0.667, adj=0.273, (0 split)\n\nNode number 54: 19 observations\n  predicted class=FALSE  expected loss=0.4210526  P(node) =0.01188243\n    class counts:    11     8\n   probabilities: 0.579 0.421 \n\nNode number 55: 10 observations\n  predicted class=TRUE   expected loss=0.1  P(node) =0.006253909\n    class counts:     1     9\n   probabilities: 0.100 0.900 \n\nNode number 94: 26 observations,    complexity param=0.01382488\n  predicted class=FALSE  expected loss=0.4615385  P(node) =0.01626016\n    class counts:    14    12\n   probabilities: 0.538 0.462 \n  left son=188 (13 obs) right son=189 (13 obs)\n  Primary splits:\n      citric.acid      &lt; 0.525 to the left,  improve=2.7692310, (0 missing)\n      sulphates        &lt; 0.775 to the right, improve=2.7531420, (0 missing)\n      alcohol          &lt; 10.95 to the right, improve=1.5766720, (0 missing)\n      volatile.acidity &lt; 0.315 to the right, improve=0.7326007, (0 missing)\n  Surrogate splits:\n      sulphates        &lt; 0.845 to the right, agree=0.692, adj=0.385, (0 split)\n      volatile.acidity &lt; 0.295 to the left,  agree=0.654, adj=0.308, (0 split)\n      alcohol          &lt; 10.85 to the right, agree=0.615, adj=0.231, (0 split)\n\nNode number 95: 22 observations\n  predicted class=TRUE   expected loss=0.1818182  P(node) =0.0137586\n    class counts:     4    18\n   probabilities: 0.182 0.818 \n\nNode number 188: 13 observations\n  predicted class=FALSE  expected loss=0.2307692  P(node) =0.008130081\n    class counts:    10     3\n   probabilities: 0.769 0.231 \n\nNode number 189: 13 observations\n  predicted class=TRUE   expected loss=0.3076923  P(node) =0.008130081\n    class counts:     4     9\n   probabilities: 0.308 0.692 \n\n\n\nrpart.plot(dtree2)\n\n\n\npred_dtree2 &lt;- predict(dtree2, newdata=wine, type=\"class\")\n\nWe will use confusion matrices to evaluate the two decision trees.\n\ntable(wine$high, pred_dtree1)\n\n       pred_dtree1\n        FALSE TRUE\n  FALSE  1343   39\n  TRUE    142   75\n\ntable(wine$high, pred_dtree2)\n\n       pred_dtree2\n        FALSE TRUE\n  FALSE  1334   48\n  TRUE    106  111\n\n\nNext, we will calculate the accuracy using \\[Accuracy = \\frac{True Positive + True Negative}  {(True Positive + True Negative + FalsePositve + FalseNegative)} \\]\n\naccuracy1 &lt;- (1343 + 75) / (1343 + 75 + 39 + 142)\nprint(accuracy1)\n\n[1] 0.8868043\n\n\n\naccuracy2 &lt;- (1334 + 111) / (1334 + 111 + 48 + 106)\nprint(accuracy2)\n\n[1] 0.9036898\n\n\nThe second model with more of the variables has a better accuracy of 90.36%.\nIn conclusion, classification in machine learning is a fundamental task with wide-ranging applications. By employing various algorithms and evaluating their performance, data scientists and machine learning practitioners can build robust models capable of making accurate predictions."
  }
]